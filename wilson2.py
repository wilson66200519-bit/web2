import streamlit as st
import google.generativeai as genai
import pandas as pd
import json
import time
import requests
import re
from urllib.parse import urljoin, urlparse
from tavily import TavilyClient

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="è¶…ç´šæ¥­å‹™é–‹ç™¼åŠ©æ‰‹ (å…¨çŸ¥å…¨èƒ½ç‰ˆ)", layout="wide")
st.title("ğŸ•µï¸â€â™‚ï¸ å…¨è‡ªå‹•å®¢æˆ¶åå–®æœé›†å™¨ (æ·±å±¤æŒ–æ˜+æ‘˜è¦åˆ†æ)")
st.markdown("""
### ğŸš€ é€™æ˜¯æœ€çµ‚çš„å¼·åŠ›ç‰ˆæœ¬ï¼š
1. **æ“´å¤§æœç´¢ç¯„åœ**ï¼šä¸åƒ…æ‰¾ã€Œè¯çµ¡æˆ‘å€‘ã€ï¼Œé‚„æœƒæ‰¾ã€Œæœå‹™æ“šé»ã€ã€ã€Œå…¬å¸ç°¡ä»‹ã€ï¼ŒæŠŠè—åœ¨æ·±è™•çš„ Email æŒ–å‡ºä¾†ã€‚
2. **æ‘˜è¦åˆ†æ**ï¼šå¼·åˆ¶ AI é–±è®€æœå°‹å¼•æ“çš„é è¦½æ–‡å­— (Snippet)ï¼Œå¾€å¾€ Email å°±è—åœ¨é‚£è£¡ã€‚
3. **æ™ºæ…§åˆ†é¡**ï¼šé›»è©±ã€çµ±ç·¨ã€å‚³çœŸ è‡ªå‹•æ­¸ä½ã€‚
""")

# --- 2. å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    if "GEMINI_API_KEY" in st.secrets:
        gemini_api_key = st.secrets["GEMINI_API_KEY"]
        st.success("âœ… å·²è®€å– Gemini Key")
    else:
        gemini_api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")

    if "TAVILY_API_KEY" in st.secrets:
        tavily_api_key = st.secrets["TAVILY_API_KEY"]
        st.success("âœ… å·²è®€å– Tavily Key")
    else:
        tavily_api_key = st.text_input("è¼¸å…¥ Tavily API Key", type="password")

    num_results = st.slider("æœå°‹æ•¸é‡", 3, 20, 5) 
    debug_mode = st.toggle("é¡¯ç¤ºå¾Œå°è™•ç†éç¨‹", value=True)

# --- 3. æ ¸å¿ƒå·¥å…· ---

def get_root_url(url):
    """ å¼·åˆ¶è½‰å›é¦–é  """
    if not url: return ""
    try:
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    except:
        return url

def find_contact_link(markdown_text, root_url):
    """ 
    å°‹æ‰¾å…§é é€£çµ (æ“´å……é—œéµå­—ç‰ˆ) 
    é‡å°å‚³ç”¢ç¶²é ï¼Œå¢åŠ ã€Œæ“šé»ã€ã€ã€Œç°¡ä»‹ã€ã€ã€Œæœå‹™ã€ç­‰é—œéµå­—
    """
    links = re.findall(r'\[(.*?)\]\((.*?)\)', markdown_text)
    
    # [é—œéµä¿®æ”¹] æ“´å¤§é—œéµå­—æ¸…å–®
    keywords = [
        "è¯çµ¡", "contact", "about", "é—œæ–¼", "support", "inquiry", "è©¢åƒ¹", 
        "æœå‹™", "service", "map", "location", "æ“šé»", "ç‡Ÿæ¥­", "profile", "ç°¡ä»‹"
    ]
    
    for text, link in links:
        for kw in keywords:
            if kw in text.lower():
                full_link = urljoin(root_url, link)
                return full_link, text
    return None, None

def fetch_content_smart(url, fallback_content=""):
    """ æ™ºæ…§æŠ“å–æµç¨‹ """
    if fallback_content is None: fallback_content = ""
    
    combined_content = ""
    source_log = []

    root_url = get_root_url(url)
    jina_url = f"https://r.jina.ai/{root_url}"
    
    try:
        resp = requests.get(jina_url, timeout=10)
        if resp.status_code == 200 and len(resp.text) > 200:
            homepage_text = resp.text
            combined_content += f"\n=== é¦–é å…§å®¹ ===\n{homepage_text[:20000]}"
            source_log.append("é¦–é ")
            
            # å¦‚æœé¦–é æ²’çœ‹åˆ° Emailï¼Œå˜—è©¦æ‰¾å…§é 
            if "@" not in homepage_text:
                contact_link, link_text = find_contact_link(homepage_text, root_url)
                if contact_link:
                    source_log.append(f"å…§é ({link_text})")
                    jina_contact_url = f"https://r.jina.ai/{contact_link}"
                    resp_inner = requests.get(jina_contact_url, timeout=10)
                    if resp_inner.status_code == 200:
                        combined_content += f"\n=== {link_text} ===\n{resp_inner.text[:20000]}"
        else:
            if len(fallback_content) > 50:
                combined_content = fallback_content
                source_log.append("åº«å­˜")
                
    except:
        if len(fallback_content) > 50:
            combined_content = fallback_content
            source_log.append("åº«å­˜(æ•‘æ´)")

    return combined_content, " + ".join(source_log)

def regex_heavy_duty(text):
    """ å¼·åŠ›æƒæ + æ™ºæ…§åˆ†é¡ """
    if not text: return [], [], [], []
    
    text_clean = " ".join(text.split())
    
    # 1. æŠ“ Email
    emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', text_clean)
    mailto_emails = re.findall(r'mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', text)
    all_emails = list(set(emails + mailto_emails))

    # 2. æŠ“ å‚³çœŸ (Fax)
    # æ”¯æ´ Fax/Tel é€™ç¨®æ··åˆå¯«æ³•
    fax_patterns = [
        r'(?:Fax|FAX|å‚³çœŸ|Facsimile|F\.|F:)[\s:ï¼š\.]*(\(?0\d{1,2}\)?[\s\-]?[0-9-]{6,15})',
        r'(?:Tel\/Fax|TEL\/FAX)[\s:ï¼š\.]*(\(?0\d{1,2}\)?[\s\-]?[0-9-]{6,15})'
    ]
    
    faxes = []
    for pattern in fax_patterns:
        found = re.findall(pattern, text)
        faxes.extend(found)
    faxes = list(set(faxes))
    
    # 3. æŠ“ æ‰€æœ‰æ•¸å­—ä¸² (ç–‘ä¼¼é›»è©±æˆ–çµ±ç·¨)
    raw_numbers = re.findall(r'(?:\(?0\d{1,2}\)?[\s\-]?)?\d{3,4}[\s\-]?\d{3,4}', text_clean)
    
    phones = []
    tax_ids = []
    
    for num in list(set(raw_numbers)):
        clean_num = re.sub(r'\D', '', num)
        
        # éæ¿¾å‚³çœŸ
        is_fax = False
        for f in faxes:
            if clean_num in re.sub(r'\D', '', f):
                is_fax = True
                break
        if is_fax: continue

        # åˆ†é¡é‚è¼¯
        if len(clean_num) >= 8:
            if clean_num.startswith('0'): 
                phones.append(num)
            elif len(clean_num) == 8:
                tax_ids.append(clean_num)
            else:
                phones.append(num)

    return all_emails, phones, faxes, tax_ids

# --- 4. AI åˆ†æå‡½æ•¸ ---

def extract_contact_info(content, url, model, snippet_content=""):
    # [é—œéµä¿®æ”¹] çµ„åˆæ‰€æœ‰æ–‡æœ¬ï¼šç¶²é å…§å®¹ + æœå°‹æ‘˜è¦
    # è®“ regex ä¹Ÿèƒ½æƒæåˆ°æ‘˜è¦è£¡çš„ Email
    full_scan_text = content + "\n=== æœå°‹å¼•æ“æ‘˜è¦ ===\n" + snippet_content
    
    emails, phones, faxes, tax_ids = regex_heavy_duty(full_scan_text)
    
    try:
        backup_info = f"Email: {emails[:3]}, é›»è©±: {phones[:3]}, å‚³çœŸ: {faxes[:2]}"
        prompt = f"""
        ä½ æ˜¯ä¸€å€‹è³‡æ–™æå–æ©Ÿå™¨äººã€‚è«‹åˆ†æç¶²é å…§å®¹èˆ‡æœå°‹æ‘˜è¦ï¼Œæ‰¾å‡ºè¯çµ¡æ–¹å¼ã€‚
        
        ç¶²å€ï¼š{url}
        åƒè€ƒæ•¸æ“š(Regexæƒæ)ï¼š{backup_info}

        ã€é‡è¦ã€‘æœå°‹å¼•æ“æ‘˜è¦ (Snippet)ï¼š
        {snippet_content}

        ç¶²é å…§å®¹æ‘˜è¦ï¼š
        {content[:40000]} 
        
        è«‹å›å‚³ JSONï¼š
        {{
            "å…¬å¸åç¨±": "...", 
            "é›»è©±": "...", 
            "Email": "...",
            "å‚³çœŸ": "...",
            "ç¶²å€": "{url}"
        }}
        """
        response = model.generate_content(prompt)
        txt = response.text.strip()
        
        if "```json" in txt:
            txt = txt.split("```json")[1].split("```")[0]
        elif "```" in txt:
            txt = txt.split("```")[0]
            
        data = json.loads(txt)

        # --- å¼·åŠ›å›å¡«æ©Ÿåˆ¶ ---
        # å¦‚æœ AI æ²’å¡«ï¼Œå°±ç”¨ Regex æƒåˆ°çš„è³‡æ–™è£œ (åŒ…å«å¾ Snippet æƒåˆ°çš„)
        if (not data.get("Email") or str(data.get("Email")).lower() in ["none", "", "null"]) and emails:
            data["Email"] = ", ".join(emails[:2])
            
        if (not data.get("é›»è©±") or str(data.get("é›»è©±")).lower() in ["none", "", "null"]) and phones:
            data["é›»è©±"] = ", ".join(phones[:2])
            
        if (not data.get("å‚³çœŸ") or str(data.get("å‚³çœŸ")).lower() in ["none", "", "null"]) and faxes:
            data["å‚³çœŸ"] = faxes[0]

        if tax_ids:
            data["çµ±ç·¨"] = ", ".join(tax_ids[:1])
        else:
            data["çµ±ç·¨"] = ""

        return data

    except:
        return {
            "å…¬å¸åç¨±": "ERROR", 
            "é›»è©±": ", ".join(phones[:2]), 
            "Email": ", ".join(emails[:2]), 
            "å‚³çœŸ": faxes[0] if faxes else "",
            "çµ±ç·¨": ", ".join(tax_ids[:1]),
            "ç¶²å€": url
        }

# --- 5. ä¸»ç¨‹å¼ ---
keyword = st.text_input("ğŸ” è«‹è¼¸å…¥æœå°‹é—œéµå­—", value="å»¢æ°´å›æ”¶ç³»çµ± å…¬å¸")

if st.button("é–‹å§‹æœå°‹èˆ‡åˆ†æ"):
    if not gemini_api_key or not tavily_api_key:
        st.error("âŒ è«‹è¼¸å…¥ API Key")
    else:
        genai.configure(api_key=gemini_api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        tavily = TavilyClient(api_key=tavily_api_key)
        
        status_box = st.status("ğŸš€ å…¨çŸ¥å…¨èƒ½æ¨¡å¼å•Ÿå‹•...", expanded=True)
        results_list = []
        
        try:
            status_box.write(f"æ­£åœ¨æœå°‹ï¼š{keyword}...")
            response = tavily.search(query=keyword, max_results=num_results, include_raw_content=True)
            search_results = response.get('results', [])
            
            if not search_results:
                status_box.error("æ‰¾ä¸åˆ°ç¶²å€")
            else:
                progress_bar = st.progress(0)
                
                for i, item in enumerate(search_results):
                    try:
                        url = item.get('url', 'ç„¡ç¶²å€')
                        title = item.get('title', 'ç„¡æ¨™é¡Œ')
                        tavily_raw = item.get('raw_content') or ""
                        tavily_snippet = item.get('content') or "" # å–å¾—æœå°‹æ‘˜è¦
                        
                        status_box.write(f"({i+1}/{len(search_results)}) åˆ†æï¼š{title}")
                        
                        content, source_log = fetch_content_smart(url, fallback_content=tavily_raw)
                        
                        if debug_mode:
                            with st.expander(f"ğŸ” è¿½è¹¤è·¯å¾‘: {source_log}"):
                                # é è¦½ä¸€ä¸‹æœ‰æ²’æœ‰æŠ“åˆ°
                                emails, _, _, _ = regex_heavy_duty(content + "\n" + tavily_snippet)
                                st.write(f"ç›®å‰æƒæåˆ°çš„ Email æ•¸é‡: {len(emails)}")
                        
                        if len(content) > 50 or len(tavily_snippet) > 20:
                            # å‚³å…¥ snippet_content çµ¦ AI åˆ†æ
                            data = extract_contact_info(content, url, model, snippet_content=tavily_snippet)
                            
                            name = str(data.get("å…¬å¸åç¨±", ""))
                            if name in ["ERROR", "None"] or "å¤±æ•—" in name:
                                data["å…¬å¸åç¨±"] = title
                            
                            results_list.append(data)
                        else:
                            pass
                            
                    except:
                        pass
                        
                    progress_bar.progress((i + 1) / len(search_results))
                    time.sleep(1)

                status_box.update(label="ğŸ‰ æœé›†å®Œæˆï¼", state="complete", expanded=False)
                
                if results_list:
                    df = pd.DataFrame(results_list)
                    cols = ["å…¬å¸åç¨±", "çµ±ç·¨", "é›»è©±", "Email", "å‚³çœŸ", "ç¶²å€"]
                    
                    for c in cols:
                        if c not in df.columns: df[c] = ""
                    df = df[cols]

                    st.dataframe(df)
                    
                    excel_file = "leads_omniscient.xlsx"
                    df.to_excel(excel_file, index=False)
                    with open(excel_file, "rb") as f:
                        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel åå–®", f, file_name=f"{keyword}_å…¨çŸ¥åå–®.xlsx")

        except Exception as e:
            st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")