import streamlit as st
import pandas as pd
from tavily import TavilyClient
import google.generativeai as genai
import time
import json

# --- 1. åŸºç¤è¨­å®š ---
st.set_page_config(page_title="è¶…ç´šåå–®æœé›†å™¨ (Secretsç‰ˆ)", layout="wide")
st.title("ğŸ“Š ä¼æ¥­åå–®è‡ªå‹•æœé›† (å·²ä¸²æ¥ Secrets)")
st.markdown("å°ˆé–€è§£æ±ºã€Œç¯„åœå¤ªå»£ã€çš„å•é¡Œï¼šAI æœƒè‡ªå‹•å°‡å¤§é—œéµå­—æ‹†è§£æˆæ•¸åå€‹ç²¾æº–æœå°‹è©ã€‚")

# --- 2. è®€å– Secrets ---
# å˜—è©¦å¾ Streamlit Secrets è®€å–é‡‘é‘°
try:
    tavily_api_key = st.secrets["TAVILY_API_KEY"]
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    st.success("âœ… å·²æˆåŠŸå¾ Secrets è¼‰å…¥ API Keys")
except FileNotFoundError:
    st.error("âŒ æ‰¾ä¸åˆ° secrets.toml æ–‡ä»¶ï¼Œè«‹ç¢ºèªæ˜¯å¦å·²å»ºç«‹ .streamlit/secrets.toml")
    st.stop()
except KeyError as e:
    st.error(f"âŒ Secrets è¨­å®šæª”ä¸­ç¼ºå°‘è®Šæ•¸ï¼š{e}ï¼Œè«‹ç¢ºèªè®Šæ•¸åç¨±æ˜¯å¦ç‚º TAVILY_API_KEY èˆ‡ GEMINI_API_KEY")
    st.stop()

# --- 3. å´é‚Šæ¬„åƒæ•¸ ---
with st.sidebar:
    st.header("âš™ï¸ æœå°‹è¨­å®š")
    
    # ç¯„åœ 100 - 500
    target_limit = st.slider("ğŸ¯ ç›®æ¨™è³‡æ–™ç­†æ•¸", min_value=100, max_value=500, value=100, step=50)
    st.info(f"è¨­å®š {target_limit} ç­†æ™‚ï¼ŒAI å°‡æœƒè‡ªå‹•è¦åŠƒç´„ {int(target_limit/10)+5} çµ„ä¸åŒçš„é—œéµå­—é€²è¡Œåœ°æ¯¯å¼æœç´¢ã€‚")

# --- 4. ä¸»ç•«é¢ ---
col1, col2 = st.columns([3, 1])
with col1:
    search_query = st.text_input("æœå°‹é—œéµå­— (ä¾‹å¦‚ï¼šå»ºç¯‰æ¥­ã€é£Ÿå“æ¥­ã€å»¢æ°´è™•ç†)", value="å»¢æ°´å›æ”¶ç³»çµ±")
with col2:
    st.write(" ") 
    st.write(" ")
    start_btn = st.button("ğŸš€ AI è¦åŠƒä¸¦åŸ·è¡Œ", type="primary", use_container_width=True)

# --- 5. åŸ·è¡Œé‚è¼¯ ---
if start_btn:

    # åˆå§‹åŒ– API
    tavily = TavilyClient(api_key=tavily_api_key)
    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel('gemini-1.5-pro')

    # ==========================
    # éšæ®µé›¶ï¼šAI é—œéµå­—è£‚è®Š
    # ==========================
    status_box = st.status("ğŸ§  AI æ­£åœ¨åˆ†æç”¢æ¥­çµæ§‹ä¸¦è¦åŠƒæœå°‹ç­–ç•¥...", expanded=True)
    
    # è¨ˆç®—éœ€è¦å¤šå°‘å€‹æœå°‹è©
    needed_queries = int(target_limit / 10) + 5
    
    plan_prompt = f"""
    ä½¿ç”¨è€…æƒ³è¦æœå°‹é—œæ–¼ã€Œ{search_query}ã€çš„ä¼æ¥­åå–®ã€‚
    å› ç‚ºç¯„åœå¾ˆå»£ï¼Œè«‹ä½ å¹«æˆ‘æ‹†è§£å‡º {needed_queries} å€‹ã€Œå…·é«”ä¸”å¤šæ¨£åŒ–ã€çš„æœå°‹é—œéµå­—ï¼Œä»¥ä¾¿æ‰¾å‡ºè©²é ˜åŸŸä¸Šä¸­ä¸‹æ¸¸çš„ä¸åŒå…¬å¸ã€‚
    
    è«‹åŒ…å«ï¼š
    1. å…·é«”çš„è¨­å‚™åç¨± (ä¾‹å¦‚ï¼šROé€†æ»²é€ã€æ±™æ³¥å£“æ¿¾æ©Ÿ)
    2. å…·é«”çš„æœå‹™é¡å‹ (ä¾‹å¦‚ï¼šä»£æ“ã€ç’°ä¿å·¥ç¨‹ã€æª¢æ¸¬)
    3. ç›¸é—œçš„ä¾›æ‡‰éˆè§’è‰² (ä¾‹å¦‚ï¼šè£½é€ å•†ã€ä»£ç†å•†ã€ç¶“éŠ·å•†)
    4. çµåˆå°ç£ä¸»è¦å·¥æ¥­å€æˆ–åœ°å€ (ä¾‹å¦‚ï¼šç«¹ç§‘ å»¢æ°´è™•ç†ã€é«˜é›„ ç’°ä¿å…¬å¸)

    è«‹ç›´æ¥å›å‚³ä¸€å€‹ JSON String Arrayï¼Œä¾‹å¦‚ï¼š
    ["{search_query} è¨­å‚™å•†", "{search_query} å·¥ç¨‹å…¬å¸", "ç‰¹å®šæŠ€è¡“ å» å•†"...]
    
    æ³¨æ„ï¼šåªå›å‚³ JSON Arrayï¼Œä¸è¦æœ‰ Markdownã€‚
    """
    
    try:
        plan_res = model.generate_content(plan_prompt)
        plan_text = plan_res.text.replace("```json", "").replace("```", "").strip()
        search_keywords = json.loads(plan_text)
        
        status_box.write(f"âœ… ç­–ç•¥è¦åŠƒå®Œæˆï¼AI ç”Ÿæˆäº† {len(search_keywords)} çµ„ç²¾æº–æœå°‹è©ï¼š")
        status_box.json(search_keywords)
        
    except Exception as e:
        status_box.warning(f"AI è¦åŠƒå¤±æ•—ï¼Œåˆ‡æ›å›é è¨­ç­–ç•¥: {e}")
        search_keywords = [f"{search_query} {s}" for s in ["å» å•†", "å…¬å¸", "ä¾›æ‡‰å•†", "å·¥ç¨‹", "è¨­å‚™", "å°åŒ—", "å°ä¸­", "é«˜é›„"]]

    # ==========================
    # éšæ®µä¸€ï¼šä¾æ“š AI ç­–ç•¥é€²è¡Œæœå°‹
    # ==========================
    status_box.write("ğŸ“¡ é–‹å§‹åŸ·è¡Œå¤šåŸ·è¡Œç·’æœå°‹...")
    
    raw_results = []
    seen_urls = set()
    progress_bar = st.progress(0)
    
    # è¿´åœˆæŠ“å–
    for i, query in enumerate(search_keywords):
        if len(raw_results) >= target_limit:
            break
            
        status_box.write(f"ğŸ” ({len(raw_results)}/{target_limit}) æ­£åœ¨æœå°‹ï¼š**{query}**")
        
        try:
            response = tavily.search(
                query=query,
                max_results=20, 
                search_depth="advanced"
            )
            
            items_found = 0
            for item in response.get('results', []):
                url = item.get('url')
                if url and url not in seen_urls:
                    raw_results.append(item) 
                    seen_urls.add(url)
                    items_found += 1
            
            time.sleep(0.5) 
            
        except Exception:
            continue
            
        search_progress = min(len(raw_results) / target_limit, 1.0) * 0.7
        progress_bar.progress(search_progress)

    final_raw_data = raw_results[:target_limit]
    status_box.write(f"âœ… æœå°‹å®Œæˆï¼å…±å–å¾— {len(final_raw_data)} ç­†è³‡æ–™ã€‚é–‹å§‹ AI æ¬„ä½èƒå–...")

    # ==========================
    # éšæ®µäºŒï¼šGemini æ•´ç†æ¬„ä½
    # ==========================
    parsed_data = []
    batch_size = 15 
    
    if len(final_raw_data) > 0:
        total_batches = (len(final_raw_data) + batch_size - 1) // batch_size
        
        for i in range(0, len(final_raw_data), batch_size):
            batch = final_raw_data[i:i+batch_size]
            
            current_batch_idx = i // batch_size
            prog = 0.7 + 0.3 * (current_batch_idx / total_batches)
            progress_bar.progress(min(prog, 0.99))
            
            try:
                batch_json = json.dumps(batch, ensure_ascii=False)
                
                prompt = f"""
                è«‹å¾ JSON è³‡æ–™ä¸­æå–å…¬å¸è¯çµ¡è³‡è¨Šã€‚
                è¼¸å‡º JSON Arrayï¼ŒåŒ…å«ï¼š
                1. "å…¬å¸åç¨±"
                2. "Email" (ç„¡å‰‡ç©º)
                3. "å‚³çœŸ" (ç„¡å‰‡ç©º)
                4. "é›»è©±" (ç„¡å‰‡ç©º)
                5. "ç¶²å€" (ä½¿ç”¨ url)

                åŸå§‹è³‡æ–™:
                {batch_json}
                """
                
                res = model.generate_content(prompt)
                clean_json = res.text.replace("```json", "").replace("```", "").strip()
                
                try:
                    batch_result = json.loads(clean_json)
                    parsed_data.extend(batch_result)
                except:
                    for item in batch:
                        parsed_data.append({"å…¬å¸åç¨±": item.get('title'), "Email":"", "å‚³çœŸ":"", "é›»è©±":"", "ç¶²å€": item.get('url')})
                
            except:
                for item in batch:
                    parsed_data.append({"å…¬å¸åç¨±": item.get('title'), "Email":"", "å‚³çœŸ":"", "é›»è©±":"", "ç¶²å€": item.get('url')})
            
            time.sleep(1.0) 

    progress_bar.progress(1.0)
    status_box.update(label="ğŸ‰ è™•ç†å®Œæˆï¼", state="complete", expanded=False)

    # ==========================
    # éšæ®µä¸‰ï¼šç”¢å‡º Excel
    # ==========================
    df = pd.DataFrame(parsed_data)
    target_cols = ["å…¬å¸åç¨±", "Email", "å‚³çœŸ", "ç¶²å€", "é›»è©±"]
    for col in target_cols:
        if col not in df.columns: df[col] = ""
    df = df[target_cols]

    st.subheader(f"æª”æ¡ˆé è¦½ (å…± {len(df)} ç­†)")
    st.dataframe(df.head(), use_container_width=True)
    
    csv = df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è¼‰ Excel æª”æ¡ˆ ({len(df)}ç­†è³‡æ–™.csv)",
        data=csv,
        file_name=f'{search_query}_åå–®.csv',
        mime='text/csv',
        type="primary"
    )