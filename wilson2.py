import streamlit as st
import pandas as pd
from tavily import TavilyClient
import google.generativeai as genai
import time
import json
import re

# ==========================================
# ğŸ”‘ è¨­å®š API Key (å„ªå…ˆå¾ Secrets è®€å–)
# ==========================================
try:
    tavily_api_key = st.secrets["TAVILY_API_KEY"]
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    api_source = "Secrets"
except:
    tavily_api_key = ""
    gemini_api_key = ""
    api_source = "None"

# --- 1. åŸºç¤è¨­å®š ---
st.set_page_config(page_title="ä¼æ¥­åå–®æœé›†å™¨ (è¬ç”¨ç‰ˆ)", layout="wide")
st.title("ğŸ“Š ä¼æ¥­åå–®è‡ªå‹•æœé›† (è‡ªå‹•åµæ¸¬æ¨¡å‹ç‰ˆ)")
st.markdown("å·²åŠ å…¥ã€Œè‡ªå‹•åµæ¸¬ã€åŠŸèƒ½ï¼Œç³»çµ±æœƒè‡ªå‹•å°‹æ‰¾æ‚¨å¸³è™Ÿå¯ç”¨çš„ Gemini æ¨¡å‹ï¼Œä¸å†å ±éŒ¯ã€‚")

if api_source == "Secrets":
    st.success("âœ… å·²æˆåŠŸå¾ Secrets è¼‰å…¥ API Keys")
else:
    st.warning("âš ï¸ æœªåµæ¸¬åˆ° Secretsï¼Œè«‹ç¢ºèªä»£ç¢¼ä¸­æ˜¯å¦å·²å¡«å…¥ API Keyã€‚")

# --- 2. å´é‚Šæ¬„åƒæ•¸ ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    if not tavily_api_key:
        tavily_api_key = st.text_input("Tavily API Key", type="password")
    if not gemini_api_key:
        gemini_api_key = st.text_input("Gemini API Key", type="password")
        
    st.divider()
    target_limit = st.slider("ğŸ¯ ç›®æ¨™è³‡æ–™ç­†æ•¸", 100, 500, 100, 50)
    st.info(f"ç›®æ¨™ï¼š{target_limit} ç­†ã€‚")

# --- 3. ä¸»ç•«é¢ ---
col1, col2 = st.columns([3, 1])
with col1:
    search_query = st.text_input("æœå°‹é—œéµå­—", value="å»¢æ°´å›æ”¶ç³»çµ±")
with col2:
    st.write(" ") 
    st.write(" ")
    start_btn = st.button("ğŸš€ é–‹å§‹åŸ·è¡Œ", type="primary", use_container_width=True)

# --- 4. åŸ·è¡Œé‚è¼¯ ---
if start_btn:
    if not tavily_api_key or not gemini_api_key:
        st.error("âŒ ç¼ºå°‘ API Keyï¼")
        st.stop()

    # åˆå§‹åŒ– Tavily
    tavily = TavilyClient(api_key=tavily_api_key)
    
    # åˆå§‹åŒ– Gemini ä¸¦è‡ªå‹•å°‹æ‰¾å¯ç”¨æ¨¡å‹
    genai.configure(api_key=gemini_api_key)
    
    status_box = st.status("ğŸ”§ æ­£åœ¨æª¢æ¸¬å¯ç”¨çš„ AI æ¨¡å‹...", expanded=True)
    
    # === è‡ªå‹•åµæ¸¬æ¨¡å‹é‚è¼¯ (é—œéµä¿®å¾©) ===
    valid_model_name = "gemini-pro" # æœ€ä¿éšªçš„é è¨­å€¼
    try:
        available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
        
        # å„ªå…ˆé †åºï¼šFlash > 1.5 Pro > 1.0 Pro
        if any('flash' in m for m in available_models):
            valid_model_name = next(m for m in available_models if 'flash' in m)
        elif any('1.5-pro' in m for m in available_models):
            valid_model_name = next(m for m in available_models if '1.5-pro' in m)
        elif 'models/gemini-pro' in available_models:
            valid_model_name = 'models/gemini-pro'
            
        status_box.write(f"âœ… æˆåŠŸé€£ç·šï¼å°‡ä½¿ç”¨æ¨¡å‹ï¼š**{valid_model_name}**")
        model = genai.GenerativeModel(valid_model_name)
        
    except Exception as e:
        status_box.warning(f"åµæ¸¬æ¨¡å‹åˆ—è¡¨å¤±æ•—ï¼Œå˜—è©¦å¼·åˆ¶ä½¿ç”¨èˆŠç‰ˆæ¨¡å‹ gemini-proã€‚éŒ¯èª¤: {e}")
        model = genai.GenerativeModel('gemini-pro')

    # ==========================
    # éšæ®µä¸€ï¼šæœå°‹
    # ==========================
    status_box.write("ğŸš€ å•Ÿå‹•æœå°‹å¼•æ“...")
    
    suffixes = [
        " å» å•†", " å…¬å¸", " ä¾›æ‡‰å•†", " å·¥ç¨‹", " è¨­å‚™", 
        " è¯ç¹«æ–¹å¼", " é›»è©±", " ä¼æ¥­åéŒ„", " æ¨è–¦", " è§£æ±ºæ–¹æ¡ˆ",
        " å°åŒ—", " å°ä¸­", " é«˜é›„", " å°å—", " æ–°ç«¹", " æ¡ƒåœ’",
        " ç’°ä¿å·¥ç¨‹", " æ°´è™•ç†", " å»¢æ°´ä»£æ“", " æ±™æ³¥è™•ç†"
    ]
    search_keywords = [f"{search_query}{s}" for s in suffixes]
    
    if target_limit > 200:
        search_keywords *= 2
    
    raw_results = []
    seen_urls = set()
    progress_bar = st.progress(0)
    
    for i, query in enumerate(search_keywords):
        if len(raw_results) >= target_limit:
            break
        
        status_box.write(f"ğŸ” ({len(raw_results)}/{target_limit}) æ­£åœ¨æœå°‹ï¼š**{query}**")
        
        try:
            response = tavily.search(
                query=query,
                max_results=20, 
                search_depth="advanced", 
                include_domains=[] 
            )
            
            for item in response.get('results', []):
                url = item.get('url')
                if url and url not in seen_urls:
                    raw_results.append(item)
                    seen_urls.add(url)
            
            time.sleep(0.5)
            
        except Exception:
            continue
            
        progress_bar.progress(min(len(raw_results) / target_limit, 1.0) * 0.7)

    final_raw_data = raw_results[:target_limit]
    status_box.write(f"âœ… æœå°‹å®Œæˆï¼å–å¾— {len(final_raw_data)} ç­†è³‡æ–™ã€‚é–‹å§‹ AI æ™ºèƒ½èƒå–...")

    # ==========================
    # éšæ®µäºŒï¼šAI èƒå–
    # ==========================
    parsed_data = []
    batch_size = 8 # ä¿å®ˆä¸€é»ï¼Œè¨­å°ä¸€é»é¿å…å‡ºéŒ¯
    
    if len(final_raw_data) > 0:
        total_batches = (len(final_raw_data) + batch_size - 1) // batch_size
        
        for i in range(0, len(final_raw_data), batch_size):
            batch = final_raw_data[i:i+batch_size]
            
            prog = 0.7 + 0.3 * ((i // batch_size) / total_batches)
            progress_bar.progress(min(prog, 0.99))
            
            try:
                # åªå‚³æ¨™é¡Œå’Œå…§å®¹å‰ 800 å­—ï¼Œé¿å… token çˆ†ç‚¸
                mini_batch = [{"title": d['title'], "url": d['url'], "content": d.get('content', '')[:800]} for d in batch]
                batch_json = json.dumps(mini_batch, ensure_ascii=False)
                
                prompt = f"""
                ä½ æ˜¯è³‡æ–™è™•ç†æ©Ÿå™¨äººã€‚è«‹å¾ JSON ä¸­æå–å…¬å¸è¯çµ¡è³‡è¨Šã€‚
                
                æ¬„ä½è¦æ±‚ï¼š
                1. "å…¬å¸åç¨±" (å»é™¤è´…å­—)
                2. "Email" (ç„¡å‰‡ç©º)
                3. "é›»è©±" (ç„¡å‰‡ç©º)
                4. "å‚³çœŸ" (ç„¡å‰‡ç©º)
                5. "ç¶²å€" (ä½¿ç”¨ url)

                åŸå§‹è³‡æ–™:
                {batch_json}
                
                è«‹å›å‚³ JSON Arrayã€‚ä¸è¦æœ‰ Markdownã€‚
                """
                
                res = model.generate_content(prompt)
                clean_json = res.text.replace("```json", "").replace("```", "").strip()
                
                try:
                    batch_result = json.loads(clean_json)
                    parsed_data.extend(batch_result)
                except:
                    # JSON è§£æå¤±æ•—ï¼Œå›å¡«åŸºæœ¬è³‡æ–™
                    for item in batch:
                        parsed_data.append({"å…¬å¸åç¨±": item.get('title'), "Email":"", "é›»è©±":"", "å‚³çœŸ":"", "ç¶²å€": item.get('url')})

            except Exception as e:
                # AI å‘¼å«å¤±æ•—ï¼Œå›å¡«åŸºæœ¬è³‡æ–™
                for item in batch:
                    parsed_data.append({"å…¬å¸åç¨±": item.get('title'), "Email":"", "é›»è©±":"", "å‚³çœŸ":"", "ç¶²å€": item.get('url')})
            
            time.sleep(1.0)

    progress_bar.progress(1.0)
    status_box.update(label="ğŸ‰ è™•ç†å®Œæˆï¼", state="complete", expanded=False)

    # ==========================
    # éšæ®µä¸‰ï¼šç”¢å‡º
    # ==========================
    df = pd.DataFrame(parsed_data)
    
    target_cols = ["å…¬å¸åç¨±", "é›»è©±", "Email", "å‚³çœŸ", "ç¶²å€"]
    for col in target_cols:
        if col not in df.columns: df[col] = ""
    df = df[target_cols]
    
    st.subheader(f"é è¦½ (å…± {len(df)} ç­†)")
    st.dataframe(df.head(), use_container_width=True)
    
    csv = df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è¼‰ Excel æª”æ¡ˆ ({len(df)}ç­†.csv)",
        data=csv,
        file_name=f'{search_query}_åå–®_è‡ªå‹•åµæ¸¬ç‰ˆ.csv',
        mime='text/csv',
        type="primary"
    )