import streamlit as st
import google.generativeai as genai
import pandas as pd
import json
import time
import requests
import re
from urllib.parse import urljoin, urlparse
from tavily import TavilyClient

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="è¶…ç´šæ¥­å‹™é–‹ç™¼åŠ©æ‰‹ (å°ç£çµ‚æ¥µä¿®æ­£ç‰ˆ)", layout="wide")
st.title("ğŸ‡¹ğŸ‡¼ å…¨è‡ªå‹•å®¢æˆ¶åå–®å·¥å»  (å°ç£çµ‚æ¥µä¿®æ­£ç‰ˆ)")
st.markdown("""
### ğŸ›¡ï¸ ç³»çµ±ç‹€æ…‹ï¼šReady
1. **ç¶²å€å¼·å¯«æ©Ÿåˆ¶**ï¼šç„¡è«– AI æ˜¯å¦è§£ææˆåŠŸï¼Œå¼·åˆ¶å¯«å…¥ä¾†æºç¶²å€ã€‚
2. **åç¨±æš´åŠ›æ¸…æ´—**ï¼šè‡ªå‹•ç§»é™¤ SEO è´…å­—ï¼Œé‚„åŸä¹¾æ·¨å…¬å¸åã€‚
3. **çµ±ç·¨åˆ†æµ**ï¼š8 ç¢¼æ•¸å­—è‡ªå‹•æ­¸é¡ç‚ºçµ±ç·¨ï¼Œä¸¦éæ¿¾ä¸­åœ‹è™Ÿç¢¼ã€‚
4. **é›™é‡å‚™ä»½**ï¼šå„ªå…ˆä½¿ç”¨å³æ™‚çˆ¬èŸ²ï¼Œå¤±æ•—æ™‚èª¿ç”¨æœå°‹å¼•æ“åº«å­˜ã€‚
""")

# --- 2. å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ API è¨­å®š")
    try:
        gemini_api_key = st.secrets["GEMINI_API_KEY"]
        tavily_api_key = st.secrets["TAVILY_API_KEY"]
        st.success("âœ… API Key å·²å¾ Secrets è¼‰å…¥")
    except:
        gemini_api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")
        tavily_api_key = st.text_input("è¼¸å…¥ Tavily API Key", type="password")
    
    st.divider()
    target_amount = st.slider("ç›®æ¨™è³‡æ–™ç­†æ•¸", 10, 1000, 50, step=10)
    enable_hunter = st.toggle("é–‹å•Ÿã€Œè£œåˆ€è¿½æ®ºã€ (è³‡æ–™ä¸å…¨æ™‚è‡ªå‹•äºŒæœ)", value=True)
    debug_mode = st.toggle("é¡¯ç¤ºé™¤éŒ¯è¨Šæ¯", value=False)

# --- 3. æ ¸å¿ƒå·¥å…·å‡½æ•¸ ---

def get_root_url(url):
    """ å¼·åˆ¶è½‰å›é¦–é  """
    if not url: return ""
    try:
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    except:
        return url

def force_clean_name(raw_title):
    """
    æš´åŠ›æ¸…æ´—å…¬å¸åç¨±ï¼š
    ç§»é™¤ "é¦–é ", "Home", "å…¬å¸ç°¡ä»‹" åŠåˆ†éš”ç¬¦è™Ÿå¾Œçš„è´…å­—
    """
    if not raw_title: return ""
    
    # å¸¸è¦‹åˆ†éš”ç¬¦
    separators = ['|', '-', '_', ':', 'â€“']
    best_candidate = raw_title
    
    for sep in separators:
        if sep in raw_title:
            parts = raw_title.split(sep)
            # ç­–ç•¥ï¼šæ‰¾å‡ºé•·åº¦æœ€åƒå…¬å¸å (2~6å­—) ä¸”åŒ…å« "å…¬å¸/ä¼æ¥­" çš„ç‰‡æ®µ
            found = False
            for p in parts:
                p = p.strip()
                if ("å…¬å¸" in p or "å•†è¡Œ" in p or "ä¼æ¥­" in p) and len(p) < 20:
                    best_candidate = p
                    found = True
                    break
            # å¦‚æœæ²’æ‰¾åˆ°æ˜é¡¯ç‰¹å¾µï¼Œå–æœ€çŸ­ä½†é•·åº¦ > 1 çš„ç‰‡æ®µ
            if not found:
                valid_parts = [p.strip() for p in parts if len(p.strip()) > 1]
                if valid_parts:
                    best_candidate = min(valid_parts, key=len)
            break 

    # ç§»é™¤åƒåœ¾è©
    garbage = ["é¦–é ", "Home", "Index", "æ­¡è¿å…‰è‡¨", "é—œæ–¼æˆ‘å€‘", "ç”¢å“ä»‹ç´¹", "è¯çµ¡æˆ‘å€‘", "ç³»åˆ—", "å» å•†", "æ¨è–¦", "æœ‰é™å…¬å¸"]
    # æ³¨æ„ï¼šæœ‰é™å…¬å¸å…ˆä¸åˆªï¼Œä¿ç•™å®Œæ•´æ€§ï¼Œæœ€å¾Œå†çœ‹æƒ…æ³
    
    cleaned = best_candidate
    for g in ["é¦–é ", "Home", "Index"]: # çµ•å°åƒåœ¾è©
        cleaned = cleaned.replace(g, "")
        
    return cleaned.strip()

def fetch_content_robust(url, fallback_content=""):
    """ å¼·éŸŒçˆ¬å–æµç¨‹ï¼šéæ¿¾ä¸­åœ‹ç¶²åŸŸ + Jina/Tavily é›™åˆ‡æ› """
    # ğŸš« éæ¿¾éå°ç£ç¶²åŸŸ
    if ".cn" in url or "china" in url.lower() or "alibaba" in url.lower():
        return "", "éå°ç£ç¶²åŸŸ(éæ¿¾)"

    combined_content = ""
    source_log = []
    root_url = get_root_url(url)
    
    jina_url = f"https://r.jina.ai/{root_url}"
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(jina_url, headers=headers, timeout=8)
        
        # ç°¡å–®æª¢æŸ¥ç°¡é«”å­—
        if "è”ç³»æˆ‘ä»¬" in resp.text: 
            pass 

        if resp.status_code == 200 and len(resp.text) > 100:
            combined_content += f"\n=== Jinaå³æ™‚çˆ¬å– ===\n{resp.text[:15000]}"
            source_log.append("å³æ™‚çˆ¬èŸ²")
        else:
            raise Exception("Jina content too short")
            
    except Exception as e:
        # å¤±æ•—æ™‚ä½¿ç”¨å‚™ä»½
        if fallback_content and len(fallback_content) > 50:
            combined_content += f"\n=== æœå°‹å¼•æ“åº«å­˜ ===\n{fallback_content[:15000]}"
            source_log.append("åº«å­˜æ•‘æ´")
        else:
            source_log.append("æŠ“å–å¤±æ•—")

    return combined_content, " + ".join(source_log)

def regex_heavy_duty(text):
    """ 
    Regex å¼·åŠ›æƒæï¼š
    1. åš´æ ¼å€åˆ† 8 ç¢¼çµ±ç·¨ vs é›»è©±
    2. éæ¿¾ä¸­åœ‹æ‰‹æ©Ÿè™Ÿ
    """
    if not text: return [], [], [], []
    text_clean = " ".join(text.split())
    
    # Email
    emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', text_clean)
    all_emails = list(set(emails))

    # å‚³çœŸ
    fax_patterns = [r'(?:Fax|FAX|å‚³çœŸ|F\.|F:)[\s:ï¼š\.]*(\(?0\d{1,2}\)?[\s\-]?[0-9-]{6,15})']
    faxes = []
    for pattern in fax_patterns:
        faxes.extend(re.findall(pattern, text))
    faxes = list(set(faxes))
    
    # é›»è©±èˆ‡çµ±ç·¨
    raw_numbers = re.findall(r'(?:\(?0\d{1,2}\)?[\s\-]?)?\d{3,4}[\s\-]?\d{3,4}', text_clean)
    phones = []
    tax_ids = []
    
    for num in list(set(raw_numbers)):
        clean_num = re.sub(r'\D', '', num)
        
        # æ’é™¤å‚³çœŸ
        is_fax = False
        for f in faxes:
            if clean_num in re.sub(r'\D', '', f): is_fax = True; break
        if is_fax: continue

        # ğŸš« æ’é™¤ä¸­åœ‹æ‰‹æ©Ÿ (1é–‹é ­ 11ç¢¼)
        if len(clean_num) == 11 and clean_num.startswith('1'): continue
        # ğŸš« æ’é™¤ä¸­åœ‹å¸‚è©± (020, 021 é–‹é ­)
        if clean_num.startswith('020') or clean_num.startswith('021'): continue

        # âœ… çµ±ç·¨åˆ¤æ–·ï¼š8ç¢¼ï¼Œä¸”ä¸ä»¥ 0 é–‹é ­
        if len(clean_num) == 8 and not clean_num.startswith('0'):
            tax_ids.append(clean_num)
        # âœ… é›»è©±åˆ¤æ–·ï¼šå…¶é¤˜é•·åº¦
        elif len(clean_num) >= 8:
            phones.append(num)

    return all_emails, phones, faxes, tax_ids

def hunter_search(company_name, tavily_client):
    """ è£œåˆ€æœå°‹ï¼šåŠ ä¸Š 'å°ç£' """
    if not company_name or len(company_name) < 2: return ""
    query = f"{company_name} å°ç£ é›»è©± email è¯çµ¡æ–¹å¼"
    try:
        resp = tavily_client.search(query=query, max_results=3, search_depth="advanced")
        snippets = ""
        for res in resp.get('results', []):
            snippets += res.get('content', '') + "\n"
        return snippets
    except:
        return ""

def extract_contact_info(content, url, model, company_name_hint=""):
    """ Gemini AI èƒå– (ä¸è™•ç†ç¶²å€ï¼Œç¶²å€ç”±ä¸»ç¨‹å¼å¯«å…¥) """
    if "éå°ç£ç¶²åŸŸ" in content:
        return {"å…¬å¸åç¨±": force_clean_name(company_name_hint), "å‚™è¨»": "æ’é™¤(éå°ç£ç¶²åŸŸ)"}

    emails, phones, faxes, tax_ids = regex_heavy_duty(content)
    backup_info = f"é æƒæ -> Email:{emails[:1]}, é›»è©±:{phones[:1]}, çµ±ç·¨:{tax_ids[:1]}"
    
    clean_hint = force_clean_name(company_name_hint)

    # Prompt ä¸è¦æ±‚å›å‚³ç¶²å€
    prompt = f"""
    ä»»å‹™ï¼šè³‡æ–™æ¨™æº–åŒ–ã€‚
    ç›®æ¨™å…¬å¸ï¼š{clean_hint} (åŸå§‹æ¨™é¡Œ: {company_name_hint})
    åƒè€ƒæ•¸æ“šï¼š{backup_info}
    å…§å®¹æ‘˜è¦ï¼š{content[:15000]}
    
    è«‹å›å‚³ JSON:
    {{
        "å…¬å¸åç¨±": "è«‹ä¿®æ­£ç‚ºæ­£å¼å…¨å (å»é™¤ SEO è´…å­—)",
        "é›»è©±": "...", 
        "Email": "...",
        "å‚³çœŸ": "...",
        "çµ±ç·¨": "...",
        "å‚™è¨»": "..."
    }}
    è‹¥æ‰¾ä¸åˆ°è³‡æ–™ï¼Œè«‹å„ªå…ˆå¡«å…¥åƒè€ƒæ•¸æ“šã€‚
    """
    
    try:
        response = model.generate_content(prompt)
        txt = response.text.strip()
        if "```json" in txt: txt = txt.split("```json")[1].split("```")[0]
        elif "```" in txt: txt = txt.split("```")[0]
        data = json.loads(txt)
        
        # å¼·åŠ›å›å¡« (AI æ¼æ‰çš„ç”¨ Regex è£œ)
        if not data.get("Email") and emails: data["Email"] = emails[0]
        if not data.get("é›»è©±") and phones: data["é›»è©±"] = phones[0]
        if not data.get("å‚³çœŸ") and faxes: data["å‚³çœŸ"] = faxes[0]
        if not data.get("çµ±ç·¨") and tax_ids: data["çµ±ç·¨"] = tax_ids[0]
        
        # äºŒæ¬¡æ¸…æ´—åç¨±
        if len(data.get("å…¬å¸åç¨±", "")) > 15 or "-" in data.get("å…¬å¸åç¨±", ""):
            data["å…¬å¸åç¨±"] = force_clean_name(data["å…¬å¸åç¨±"])
            
        return data
    except:
        status = "âš ï¸ AIå¤±æ•— (Regexæ•‘æ´)" if (phones or emails) else "âŒ è§£æå¤±æ•—"
        return {
            "å…¬å¸åç¨±": clean_hint,
            "é›»è©±": phones[0] if phones else "",
            "Email": emails[0] if emails else "",
            "å‚³çœŸ": faxes[0] if faxes else "",
            "çµ±ç·¨": tax_ids[0] if tax_ids else "",
            "å‚™è¨»": status
        }

def generate_keywords(base_keyword, amount, model):
    """ ç”Ÿæˆç­–ç•¥ï¼šå¼·åˆ¶åŠ ä¸Š 'å°ç£' """
    num_strategies = max(3, int(amount / 15))
    prompt = f"""
    è«‹ç”Ÿæˆ {num_strategies} çµ„æœå°‹é—œéµå­—ï¼Œç›®çš„æ˜¯æœé›†ã€Œå°ç£ã€çš„ã€Œ{base_keyword}ã€å» å•†ã€‚
    è«‹ç¢ºä¿é—œéµå­—éƒ½åŒ…å« "å°ç£" æˆ–å°ç£åœ°å (å°åŒ—, å°ä¸­, é«˜é›„)ã€‚
    åªå›å‚³ JSON Array string: ["é—œéµå­—1", "é—œéµå­—2", ...]
    """
    try:
        res = model.generate_content(prompt)
        txt = res.text.strip()
        if "```json" in txt: txt = txt.split("```json")[1].split("```")[0]
        return json.loads(txt)
    except:
        return [f"å°ç£ {base_keyword}", f"å°åŒ— {base_keyword}", f"å°ä¸­ {base_keyword}", f"é«˜é›„ {base_keyword}"]

# --- 4. ä¸»åŸ·è¡Œé‚è¼¯ ---
st.subheader("ğŸ•µï¸â€â™‚ï¸ å•Ÿå‹•æ§åˆ¶å°")
keyword = st.text_input("è¼¸å…¥æ ¸å¿ƒé—œéµå­— (ç³»çµ±æœƒè‡ªå‹•é™å®šå°ç£ç¯„åœ)", value="å»¢æ°´å›æ”¶ç³»çµ±")

if st.button("ğŸš€ å•Ÿå‹•å°ç£ç²¾æº–ç‰ˆå¼•æ“"):
    if not gemini_api_key or not tavily_api_key:
        st.error("âŒ è«‹å¡«å¯« API Key")
        st.stop()
        
    genai.configure(api_key=gemini_api_key)
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        model.generate_content("test")
    except:
        model = genai.GenerativeModel('gemini-pro')
        
    tavily = TavilyClient(api_key=tavily_api_key)
    status_box = st.status("ğŸ§  è¦åŠƒå°ç£é™å®šæœå°‹ç­–ç•¥...", expanded=True)
    
    # 1. ç­–ç•¥ç”Ÿæˆ
    strategies = generate_keywords(keyword, target_amount, model)
    status_box.write(f"âœ… æœå°‹ç­–ç•¥ï¼š{strategies}")
    
    # 2. æœé›†ç¶²å€
    unique_data = {} 
    progress_bar = st.progress(0)
    status_box.write("ğŸ•¸ï¸ æ­£åœ¨éæ¿¾ä¸¦æœé›†ç¶²å€ (å«åº«å­˜å‚™ä»½)...")
    
    for idx, q in enumerate(strategies):
        if len(unique_data) >= target_amount: break
        try:
            # include_raw_content=True æ˜¯é˜²æ­¢ç©ºç™½çš„é—œéµ
            response = tavily.search(query=q, max_results=15, include_raw_content=True)
            for res in response.get('results', []):
                url = res.get('url')
                # ğŸš« ç¶²åŸŸå±¤ç´šéæ¿¾
                if url and ".cn" not in url and "alibaba" not in url and not url.endswith('.pdf'):
                    if url not in unique_data:
                        unique_data[url] = {
                            "title": res.get('title', ''),
                            "raw_content": res.get('raw_content') or res.get('content', '') 
                        }
        except: pass
        progress_bar.progress(min(len(unique_data) / target_amount, 1.0))
        time.sleep(1)

    # 3. æ·±åº¦æŒ–æ˜
    status_box.write(f"ğŸ­ é–‹å§‹è™•ç† {len(unique_data)} ç­†å°ç£å» å•†è³‡æ–™...")
    final_results = []
    process_bar = st.progress(0)
    table_preview = st.empty()
    
    target_list = list(unique_data.items())[:target_amount]
    
    for i, (url, info) in enumerate(target_list):
        title = info['title']
        raw_backup = info['raw_content']
        
        try:
            content, source = fetch_content_robust(url, fallback_content=raw_backup)
            
            # è‹¥ç¬¬ä¸€æ­¥å°±ç™¼ç¾æ˜¯éå°ç£ç¶²åŸŸï¼Œè·³é
            if "éå°ç£ç¶²åŸŸ" in source:
                continue

            # AI åˆ†æ
            data = extract_contact_info(content, url, model, company_name_hint=title)
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ ç‰©ç†å¼·åˆ¶å¯«å…¥ç¶²å€èˆ‡ä¾†æº ğŸ”¥ğŸ”¥ğŸ”¥
            data["ç¶²å€"] = url
            data["è³‡æ–™ä¾†æº"] = source
            
            # è£œåˆ€æª¢æŸ¥
            missing = []
            if not data.get("Email") or str(data.get("Email")).lower() in ["none", ""]: missing.append("Email")
            if not data.get("é›»è©±") or str(data.get("é›»è©±")).lower() in ["none", ""]: missing.append("é›»è©±")
            
            if enable_hunter and missing:
                if debug_mode: status_box.write(f"ğŸ”« {data['å…¬å¸åç¨±']} è³‡æ–™ä¸å…¨ï¼Œè£œåˆ€ä¸­...")
                hunter_data = hunter_search(data['å…¬å¸åç¨±'], tavily)
                h_emails, h_phones, h_faxes, h_tax = regex_heavy_duty(hunter_data)
                
                if "Email" in missing and h_emails: data["Email"] = h_emails[0]
                if "é›»è©±" in missing and h_phones: data["é›»è©±"] = h_phones[0]
                if not data.get("çµ±ç·¨") and h_tax: data["çµ±ç·¨"] = h_tax[0]
                
                data["å‚™è¨»"] = "ç¶“äºŒæ¬¡è£œå®Œ"
            else:
                 if not data.get("å‚™è¨»"): data["å‚™è¨»"] = "ä¸€èˆ¬"
            
            final_results.append(data)
            
            # é è¦½è¡¨æ ¼
            if i % 3 == 0:
                df_show = pd.DataFrame(final_results)
                cols = ["å…¬å¸åç¨±", "çµ±ç·¨", "é›»è©±", "Email", "ç¶²å€"]
                for c in cols: 
                    if c not in df_show.columns: df_show[c] = ""
                table_preview.dataframe(df_show[cols].tail(5))
                
        except Exception as e:
            if debug_mode: st.warning(f"Error on {title}: {e}")
            
        process_bar.progress((i+1)/len(target_list))
        time.sleep(0.5)

    # 4. è¼¸å‡º
    status_box.update(label="ğŸ‰ å®Œæˆï¼", state="complete", expanded=False)
    
    if final_results:
        df_final = pd.DataFrame(final_results)
        
        # æ¬„ä½æ•´ç†
        target_cols = ["å…¬å¸åç¨±", "çµ±ç·¨", "é›»è©±", "Email", "å‚³çœŸ", "ç¶²å€", "å‚™è¨»", "è³‡æ–™ä¾†æº"]
        for c in target_cols:
            if c not in df_final.columns: df_final[c] = ""
        df_final = df_final[target_cols].astype(str)
        
        st.success(f"å…±ç”¢å‡º {len(df_final)} ç­†å°ç£å» å•†åå–®")
        st.dataframe(df_final)
        
        # è¼¸å‡º CSV (UTF-8 BOM)
        csv = df_final.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´åå–® (CSV)",
            data=csv,
            file_name="taiwan_leads_final.csv",
            mime="text/csv"
        )