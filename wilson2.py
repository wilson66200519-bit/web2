import streamlit as st
import google.generativeai as genai
import requests
import pandas as pd
import json
import time
from duckduckgo_search import DDGS  # <--- é€™è£¡æ›æˆæ–°çš„æœå°‹å¥—ä»¶

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="è¶…ç´šæ¥­å‹™é–‹ç™¼åŠ©æ‰‹", layout="wide")
st.title("ðŸ•µï¸â€â™‚ï¸ å…¨è‡ªå‹•å®¢æˆ¶åå–®æœé›†å™¨ (DDGç‰ˆ)")
st.markdown("è¼¸å…¥é—œéµå­— (ä¾‹å¦‚ï¼š`å°åŒ— å®¤å…§è¨­è¨ˆå…¬å¸`)ï¼ŒAI è‡ªå‹•å¹«ä½ æœé›†å‰ 10 å®¶å…¬å¸çš„è¯çµ¡æ–¹å¼ã€‚")

# --- 2. å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    user_api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")
    num_results = st.slider("è¦æŠ“å¹¾å®¶å…¬å¸ï¼Ÿ", 5, 20, 10)
    
    st.info("ðŸ’¡ æœå°‹å¼•æ“Žå·²åˆ‡æ›ç‚º DuckDuckGoï¼ŒæŠ“å–æ›´ç©©å®šï¼")

# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸ ---

# A. ç”¨ AI åˆ†æžç¶²é å…§å®¹ (ç¶­æŒä¸è®Š)
def extract_contact_info(html_text, url, model):
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹è³‡æ–™æŽ¢å‹˜å°ˆå®¶ã€‚è«‹å¾žä¸‹æ–¹çš„ HTML åŽŸå§‹ç¢¼ä¸­ï¼Œæå–é€™å®¶å…¬å¸çš„è¯çµ¡è³‡è¨Šã€‚
    
    ç›®æ¨™ç¶²å€ï¼š{url}
    
    è«‹å°‹æ‰¾ä»¥ä¸‹æ¬„ä½ï¼š
    1. å…¬å¸åç¨± (Company Name) - è‹¥æ‰¾ä¸åˆ°ï¼Œè«‹å¾žç¶²é æ¨™é¡ŒæŽ¨æ¸¬
    2. é›»è©± (Phone)
    3. å‚³çœŸ (Fax) - è‹¥ç„¡å‰‡ç•™ç©º
    4. Email - è‹¥ç„¡å‰‡ç•™ç©º
    5. ç¶²å€ (URL) - å›žå‚³ï¼š{url}
    
    HTML å…§å®¹æ‘˜è¦ï¼š{html_text[:50000]} 
    
    è«‹åš´æ ¼å›žå‚³ JSON æ ¼å¼ï¼Œä¸è¦æœ‰ markdown æ¨™è¨˜ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    {{
        "å…¬å¸åç¨±": "...",
        "ç¶²å€": "...",
        "é›»è©±": "...",
        "å‚³çœŸ": "...",
        "Email": "..."
    }}
    """
    try:
        response = model.generate_content(prompt)
        clean_json = response.text.strip().replace('```json', '').replace('```', '')
        return json.loads(clean_json)
    except Exception as e:
        return {"å…¬å¸åç¨±": "è§£æžå¤±æ•—", "ç¶²å€": url, "éŒ¯èª¤è¨Šæ¯": "AI ç„¡æ³•è®€å–"}

# B. çˆ¬å–å–®ä¸€ç¶²é  (ç¶­æŒä¸è®Š)
def fetch_page_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding
        return response.text
    except:
        return None

# --- 4. ä¸»ç¨‹å¼é‚è¼¯ ---
keyword = st.text_input("ðŸ” è«‹è¼¸å…¥æœå°‹é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ä¸­ ç²¾å¯†æ©Ÿæ¢°å» ")

if st.button("é–‹å§‹æœå°‹èˆ‡åˆ†æž"):
    if not user_api_key:
        st.error("âŒ è«‹å…ˆåœ¨å·¦å´è¼¸å…¥ Gemini API Key")
    elif not keyword:
        st.warning("âš ï¸ è«‹è¼¸å…¥é—œéµå­—")
    else:
        # è¨­å®š AI
        genai.configure(api_key=user_api_key)
        # ä½¿ç”¨æœ€æ–°çš„å…è²»æ¨¡åž‹
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        status_box = st.status("ðŸš€ ä»»å‹™å•Ÿå‹•ä¸­...", expanded=True)
        results_list = []
        
        # --- ç¬¬ä¸€éšŽæ®µï¼šæ”¹ç”¨ DuckDuckGo æœå°‹ ---
        status_box.write(f"æ­£åœ¨æœå°‹ï¼š{keyword}...")
        
        search_urls = []
        try:
            with DDGS() as ddgs:
                # region='tw-tzh' ä»£è¡¨æœå°‹å°ç£åœ°å€
                ddgs_gen = ddgs.text(keyword, max_results=num_results, backend="html")
                for r in ddgs_gen:
                    search_urls.append(r['href'])
            
            if len(search_urls) == 0:
                status_box.error("æ‰¾ä¸åˆ°ä»»ä½•çµæžœï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")
            else:
                status_box.write(f"âœ… æ‰¾åˆ° {len(search_urls)} å€‹ç¶²å€ï¼Œæº–å‚™é–‹å§‹é€ä¸€åˆ†æž...")
                
                # å»ºç«‹é€²åº¦æ¢
                progress_bar = st.progress(0)
                
                # --- ç¬¬äºŒéšŽæ®µï¼šé€ä¸€çˆ¬å– ---
                for i, url in enumerate(search_urls):
                    status_box.write(f"({i+1}/{len(search_urls)}) æ­£åœ¨åˆ†æžï¼š{url}")
                    
                    # 1. æŠ“ç¶²é 
                    html_content = fetch_page_content(url)
                    
                    if html_content:
                        # 2. AI æå–
                        data = extract_contact_info(html_content, url, model)
                        results_list.append(data)
                    else:
                        results_list.append({
                            "å…¬å¸åç¨±": "ç¶²é ç„¡æ³•é–‹å•Ÿ",
                            "ç¶²å€": url,
                            "é›»è©±": "", "å‚³çœŸ": "", "Email": ""
                        })
                    
                    # æ›´æ–°é€²åº¦æ¢
                    progress_bar.progress((i + 1) / len(search_urls))
                    time.sleep(1) # ä¼‘æ¯ä¸€ä¸‹

                status_box.update(label="ðŸŽ‰ åˆ†æžå®Œæˆï¼", state="complete", expanded=False)
                
                # --- 5. é¡¯ç¤ºçµæžœèˆ‡åŒ¯å‡º ---
                if results_list:
                    df = pd.DataFrame(results_list)
                    
                    st.subheader("ðŸ“Š æœå°‹çµæžœ")
                    st.dataframe(df)
                    
                    # Excel ä¸‹è¼‰
                    excel_file = "leads_data.xlsx"
                    df.to_excel(excel_file, index=False)
                    
                    with open(excel_file, "rb") as f:
                        st.download_button(
                            label="ðŸ“¥ ä¸‹è¼‰ Excel åå–®",
                            data=f,
                            file_name=f"{keyword}_å®¢æˆ¶åå–®.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )

        except Exception as e:
            st.error(f"æœå°‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")