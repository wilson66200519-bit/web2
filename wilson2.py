import streamlit as st
import google.generativeai as genai
import pandas as pd
import json
import time
import requests
import re
from urllib.parse import urljoin, urlparse
from tavily import TavilyClient

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="è¶…ç´šæ¥­å‹™é–‹ç™¼åŠ©æ‰‹ (é›»è©±åš´æ ¼ç‰ˆ)", layout="wide")
st.title("ğŸ‡¹ğŸ‡¼ å…¨è‡ªå‹•å®¢æˆ¶åå–®å·¥å»  (é›»è©±åš´æ ¼ç‰ˆ)")
st.markdown("""
### ğŸ›¡ï¸ æ›´æ–°é‡é»ï¼š
1. **ç§»é™¤çµ±ç·¨æ¬„ä½**ï¼šç‰ˆé¢æ›´ç°¡æ½”ï¼Œå°ˆæ³¨æ–¼è¯çµ¡æ–¹å¼ã€‚
2. **é›»è©±åš´æ ¼éæ¿¾**ï¼š
    * å¿…é ˆä»¥ `0` é–‹é ­ (éæ¿¾çµ±ç·¨èˆ‡é›œè¨Š)ã€‚
    * é•·åº¦é™åˆ¶ 9~10 ç¢¼ (é–å®šå°ç£å¸‚è©±èˆ‡æ‰‹æ©Ÿ)ã€‚
    * è‡ªå‹•éæ¿¾ä¸­åœ‹å€ç¢¼ (020, 021) èˆ‡æ‰‹æ©Ÿã€‚
""")

# --- 2. å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ API è¨­å®š")
    try:
        gemini_api_key = st.secrets["GEMINI_API_KEY"]
        tavily_api_key = st.secrets["TAVILY_API_KEY"]
        st.success("âœ… API Key å·²å¾ Secrets è¼‰å…¥")
    except:
        gemini_api_key = st.text_input("è¼¸å…¥ Gemini API Key", type="password")
        tavily_api_key = st.text_input("è¼¸å…¥ Tavily API Key", type="password")
    
    st.divider()
    # åŠ å…¥é€£ç·šæ¸¬è©¦æŒ‰éˆ•
    if st.button("ğŸš‘ ç³»çµ±è‡ªæˆ‘è¨ºæ–·"):
        st.write("æ­£åœ¨æ¸¬è©¦ API é€£ç·š...")
        try:
            t_client = TavilyClient(api_key=tavily_api_key)
            t_res = t_client.search("test", max_results=1)
            st.success(f"âœ… Tavily é€£ç·šæ­£å¸¸")
        except Exception as e:
            st.error(f"âŒ Tavily é€£ç·šå¤±æ•—: {e}")
        try:
            genai.configure(api_key=gemini_api_key)
            m = genai.GenerativeModel('gemini-1.5-flash')
            r = m.generate_content("Hi")
            st.success("âœ… Gemini é€£ç·šæ­£å¸¸")
        except Exception as e:
            st.error(f"âŒ Gemini é€£ç·šå¤±æ•—: {e}")

    st.divider()
    target_amount = st.slider("ç›®æ¨™è³‡æ–™ç­†æ•¸", 10, 1000, 50, step=10)
    enable_hunter = st.toggle("é–‹å•Ÿã€Œè£œåˆ€è¿½æ®ºã€", value=True)
    debug_mode = st.toggle("é¡¯ç¤ºé™¤éŒ¯è¨Šæ¯", value=False)

# --- 3. æ ¸å¿ƒå·¥å…·å‡½æ•¸ ---

def get_root_url(url):
    """ å¼·åˆ¶è½‰å›é¦–é  """
    if not url: return ""
    try:
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    except:
        return url

def force_clean_name(raw_title):
    """ å…¬å¸åç¨±æš´åŠ›æ¸…æ´— """
    if not raw_title: return ""
    separators = ['|', '-', '_', ':', 'â€“']
    best_candidate = raw_title
    
    for sep in separators:
        if sep in raw_title:
            parts = raw_title.split(sep)
            found = False
            for p in parts:
                p = p.strip()
                if ("å…¬å¸" in p or "å•†è¡Œ" in p or "ä¼æ¥­" in p) and len(p) < 20:
                    best_candidate = p
                    found = True
                    break
            if not found:
                valid_parts = [p.strip() for p in parts if len(p.strip()) > 1]
                if valid_parts:
                    best_candidate = min(valid_parts, key=len)
            break 

    garbage = ["é¦–é ", "Home", "Index", "æ­¡è¿å…‰è‡¨", "é—œæ–¼æˆ‘å€‘", "ç”¢å“ä»‹ç´¹", "è¯çµ¡æˆ‘å€‘", "ç³»åˆ—", "å» å•†", "æ¨è–¦", "æœ‰é™å…¬å¸"]
    cleaned = best_candidate
    for g in ["é¦–é ", "Home", "Index"]: 
        cleaned = cleaned.replace(g, "")
        
    return cleaned.strip()

def fetch_content_robust(url, fallback_content=""):
    """ å¼·éŸŒçˆ¬å–æµç¨‹ """
    if ".cn" in url or "china" in url.lower() or "alibaba" in url.lower():
        return "", "éå°ç£ç¶²åŸŸ(éæ¿¾)"

    combined_content = ""
    source_log = []
    root_url = get_root_url(url)
    jina_url = f"https://r.jina.ai/{root_url}"
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(jina_url, headers=headers, timeout=8)
        if "è”ç³»æˆ‘ä»¬" in resp.text: pass 

        if resp.status_code == 200 and len(resp.text) > 100:
            combined_content += f"\n=== Jinaå³æ™‚çˆ¬å– ===\n{resp.text[:15000]}"
            source_log.append("å³æ™‚çˆ¬èŸ²")
        else:
            raise Exception("Jina fail")
    except Exception as e:
        if fallback_content and len(fallback_content) > 50:
            combined_content += f"\n=== æœå°‹å¼•æ“åº«å­˜ ===\n{fallback_content[:15000]}"
            source_log.append("åº«å­˜æ•‘æ´")
        else:
            source_log.append("æŠ“å–å¤±æ•—")

    return combined_content, " + ".join(source_log)

def regex_heavy_duty(text):
    """ 
    Regex æ¥µè‡´åš´æ ¼ç‰ˆï¼š
    1. åªæŠ“ Email èˆ‡ é›»è©± (ç§»é™¤çµ±ç·¨)
    2. é›»è©±å¿…é ˆ 0 é–‹é ­ï¼Œé•·åº¦ 9-10 ç¢¼ (å®Œç¾éæ¿¾çµ±ç·¨èˆ‡é›œè¨Š)
    """
    if not text: return [], [], [] # ç§»é™¤ tax_ids å›å‚³
    text_clean = " ".join(text.split())
    
    # Email
    emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', text_clean)
    all_emails = list(set(emails))

    # å‚³çœŸ (Fax) - å…ˆæŠ“å‡ºä¾†æ’é™¤ç”¨
    fax_patterns = [r'(?:Fax|FAX|å‚³çœŸ|F\.|F:)[\s:ï¼š\.]*(\(?0\d{1,2}\)?[\s\-]?[0-9-]{6,15})']
    faxes = []
    for pattern in fax_patterns:
        faxes.extend(re.findall(pattern, text))
    faxes = list(set(faxes))
    
    # é›»è©± - åš´æ ¼é‚è¼¯
    # å…ˆæŠ“å‡ºæ‰€æœ‰å¯èƒ½çš„æ•¸å­—ä¸² (å«æ‹¬è™Ÿèˆ‡æ©«æ§“)
    raw_numbers = re.findall(r'(?:\(?0\d{1,2}\)?[\s\-]?)?\d{3,4}[\s\-]?\d{3,4}', text_clean)
    phones = []
    
    for num in list(set(raw_numbers)):
        clean_num = re.sub(r'\D', '', num) # åªç•™æ•¸å­—
        
        # 1. æ’é™¤å‚³çœŸ
        is_fax = False
        for f in faxes:
            if clean_num in re.sub(r'\D', '', f): is_fax = True; break
        if is_fax: continue

        # 2. æ’é™¤ä¸­åœ‹æ‰‹æ©Ÿ (1é–‹é ­ 11ç¢¼)
        if len(clean_num) == 11 and clean_num.startswith('1'): continue
        
        # 3. æ’é™¤ä¸­åœ‹å¸‚è©± (020, 021)
        if clean_num.startswith('020') or clean_num.startswith('021'): continue

        # ğŸ”¥ 4. é»ƒé‡‘è¦å‰‡ï¼šå°ç£é›»è©±ä¸€å®šè¦ 0 é–‹é ­
        if not clean_num.startswith('0'):
            continue # é€™è£¡æœƒç›´æ¥æ®ºæ‰çµ±ç·¨ (å› ç‚ºçµ±ç·¨é€šå¸¸ä¸æ˜¯ 0 é–‹é ­)

        # ğŸ”¥ 5. é•·åº¦è¦å‰‡ï¼šå°ç£å¸‚è©±å«å€ç¢¼é€šå¸¸ 9 æˆ– 10 ç¢¼ï¼Œæ‰‹æ©Ÿ 10 ç¢¼
        if len(clean_num) == 9 or len(clean_num) == 10:
            phones.append(num)

    return all_emails, phones, faxes

def hunter_search(company_name, tavily_client):
    """ è£œåˆ€æœå°‹ """
    if not company_name or len(company_name) < 2: return ""
    query = f"{company_name} å°ç£ é›»è©± email è¯çµ¡æ–¹å¼"
    try:
        resp = tavily_client.search(query=query, max_results=3, search_depth="advanced")
        snippets = ""
        for res in resp.get('results', []):
            snippets += res.get('content', '') + "\n"
        return snippets
    except:
        return ""

def extract_contact_info(content, url, model, company_name_hint=""):
    """ Gemini AI èƒå– (ç§»é™¤çµ±ç·¨æ¬„ä½) """
    if "éå°ç£ç¶²åŸŸ" in content:
        return {"å…¬å¸åç¨±": force_clean_name(company_name_hint), "å‚™è¨»": "æ’é™¤(éå°ç£ç¶²åŸŸ)"}

    # Regex ä¹Ÿä¸å›å‚³çµ±ç·¨äº†
    emails, phones, faxes = regex_heavy_duty(content)
    backup_info = f"é æƒæ -> Email:{emails[:1]}, é›»è©±:{phones[:1]}"
    
    clean_hint = force_clean_name(company_name_hint)

    prompt = f"""
    ä»»å‹™ï¼šè³‡æ–™æ¨™æº–åŒ–ã€‚
    ç›®æ¨™å…¬å¸ï¼š{clean_hint} (åŸå§‹æ¨™é¡Œ: {company_name_hint})
    åƒè€ƒæ•¸æ“šï¼š{backup_info}
    å…§å®¹æ‘˜è¦ï¼š{content[:15000]}
    
    è«‹å›å‚³ JSON (åš´ç¦åŒ…å«çµ±ç·¨):
    {{
        "å…¬å¸åç¨±": "è«‹ä¿®æ­£ç‚ºæ­£å¼å…¨å (å»é™¤ SEO è´…å­—)",
        "é›»è©±": "...", 
        "Email": "...",
        "å‚³çœŸ": "...",
        "å‚™è¨»": "..."
    }}
    è‹¥æ‰¾ä¸åˆ°è³‡æ–™ï¼Œè«‹å„ªå…ˆå¡«å…¥åƒè€ƒæ•¸æ“šã€‚é›»è©±è«‹å„ªå…ˆä¿ç•™ 0 é–‹é ­çš„è™Ÿç¢¼ã€‚
    """
    
    try:
        response = model.generate_content(prompt)
        txt = response.text.strip()
        if "```json" in txt: txt = txt.split("```json")[1].split("```")[0]
        elif "```" in txt: txt = txt.split("```")[0]
        data = json.loads(txt)
        
        # å¼·åŠ›å›å¡«
        if not data.get("Email") and emails: data["Email"] = emails[0]
        if not data.get("é›»è©±") and phones: data["é›»è©±"] = phones[0]
        if not data.get("å‚³çœŸ") and faxes: data["å‚³çœŸ"] = faxes[0]
        
        # ç§»é™¤å¯èƒ½è¢« AI èª¤å¡«çš„çµ±ç·¨ key (ä¿éšªèµ·è¦‹)
        if "çµ±ç·¨" in data: del data["çµ±ç·¨"]
        
        # äºŒæ¬¡æ¸…æ´—åç¨±
        if len(data.get("å…¬å¸åç¨±", "")) > 15 or "-" in data.get("å…¬å¸åç¨±", ""):
            data["å…¬å¸åç¨±"] = force_clean_name(data["å…¬å¸åç¨±"])
            
        return data
    except:
        status = "âš ï¸ AIå¤±æ•— (Regexæ•‘æ´)" if (phones or emails) else "âŒ è§£æå¤±æ•—"
        return {
            "å…¬å¸åç¨±": clean_hint,
            "é›»è©±": phones[0] if phones else "",
            "Email": emails[0] if emails else "",
            "å‚³çœŸ": faxes[0] if faxes else "",
            "å‚™è¨»": status
        }

def generate_keywords(base_keyword, amount, model):
    """ ç”Ÿæˆç­–ç•¥ """
    num_strategies = max(3, int(amount / 15))
    prompt = f"""
    è«‹ç”Ÿæˆ {num_strategies} çµ„æœå°‹é—œéµå­—ï¼Œç›®çš„æ˜¯æœé›†ã€Œå°ç£ã€çš„ã€Œ{base_keyword}ã€å» å•†ã€‚
    è«‹ç¢ºä¿é—œéµå­—éƒ½åŒ…å« "å°ç£" æˆ–å°ç£åœ°å (å°åŒ—, å°ä¸­, é«˜é›„)ã€‚
    åªå›å‚³ JSON Array string: ["é—œéµå­—1", "é—œéµå­—2", ...]
    """
    try:
        res = model.generate_content(prompt)
        txt = res.text.strip()
        if "```json" in txt: txt = txt.split("```json")[1].split("```")[0]
        return json.loads(txt)
    except:
        return [f"å°ç£ {base_keyword}", f"å°åŒ— {base_keyword}", f"å°ä¸­ {base_keyword}", f"é«˜é›„ {base_keyword}"]

# --- 4. ä¸»åŸ·è¡Œé‚è¼¯ ---
st.subheader("ğŸ•µï¸â€â™‚ï¸ å•Ÿå‹•æ§åˆ¶å°")
keyword = st.text_input("è¼¸å…¥æ ¸å¿ƒé—œéµå­— (ç³»çµ±æœƒè‡ªå‹•é™å®šå°ç£ç¯„åœ)", value="å»¢æ°´å›æ”¶ç³»çµ±")

if st.button("ğŸš€ å•Ÿå‹•å°ç£ç²¾æº–ç‰ˆå¼•æ“"):
    if not gemini_api_key or not tavily_api_key:
        st.error("âŒ è«‹å¡«å¯« API Key")
        st.stop()
        
    genai.configure(api_key=gemini_api_key)
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        model.generate_content("test")
    except:
        model = genai.GenerativeModel('gemini-pro')
        
    tavily = TavilyClient(api_key=tavily_api_key)
    status_box = st.status("ğŸ§  è¦åŠƒå°ç£é™å®šæœå°‹ç­–ç•¥...", expanded=True)
    
    # 1. ç­–ç•¥ç”Ÿæˆ
    strategies = generate_keywords(keyword, target_amount, model)
    status_box.write(f"âœ… æœå°‹ç­–ç•¥ï¼š{strategies}")
    
    # 2. æœé›†ç¶²å€
    unique_data = {} 
    progress_bar = st.progress(0)
    status_box.write("ğŸ•¸ï¸ æ­£åœ¨éæ¿¾ä¸¦æœé›†ç¶²å€ (å«åº«å­˜å‚™ä»½)...")
    
    for idx, q in enumerate(strategies):
        if len(unique_data) >= target_amount: break
        try:
            response = tavily.search(query=q, max_results=15, include_raw_content=True)
            for res in response.get('results', []):
                url = res.get('url')
                if url and ".cn" not in url and "alibaba" not in url and not url.endswith('.pdf'):
                    if url not in unique_data:
                        unique_data[url] = {
                            "title": res.get('title', ''),
                            "raw_content": res.get('raw_content') or res.get('content', '') 
                        }
        except: pass
        progress_bar.progress(min(len(unique_data) / target_amount, 1.0))
        time.sleep(1)

    # 3. æ·±åº¦æŒ–æ˜
    status_box.write(f"ğŸ­ é–‹å§‹è™•ç† {len(unique_data)} ç­†å°ç£å» å•†è³‡æ–™...")
    final_results = []
    process_bar = st.progress(0)
    table_preview = st.empty()
    
    target_list = list(unique_data.items())[:target_amount]
    
    for i, (url, info) in enumerate(target_list):
        title = info['title']
        raw_backup = info['raw_content']
        
        try:
            content, source = fetch_content_robust(url, fallback_content=raw_backup)
            
            if "éå°ç£ç¶²åŸŸ" in source: continue

            data = extract_contact_info(content, url, model, company_name_hint=title)
            
            # å¼·åˆ¶å¯«å…¥ç¶²å€
            data["ç¶²å€"] = url
            data["è³‡æ–™ä¾†æº"] = source
            
            # è£œåˆ€æª¢æŸ¥
            missing = []
            if not data.get("Email") or str(data.get("Email")).lower() in ["none", ""]: missing.append("Email")
            if not data.get("é›»è©±") or str(data.get("é›»è©±")).lower() in ["none", ""]: missing.append("é›»è©±")
            
            if enable_hunter and missing:
                if debug_mode: status_box.write(f"ğŸ”« {data['å…¬å¸åç¨±']} è³‡æ–™ä¸å…¨ï¼Œè£œåˆ€ä¸­...")
                hunter_data = hunter_search(data['å…¬å¸åç¨±'], tavily)
                h_emails, h_phones, h_faxes = regex_heavy_duty(hunter_data) # ä¸æ¥ tax_ids äº†
                
                if "Email" in missing and h_emails: data["Email"] = h_emails[0]
                if "é›»è©±" in missing and h_phones: data["é›»è©±"] = h_phones[0]
                
                data["å‚™è¨»"] = "ç¶“äºŒæ¬¡è£œå®Œ"
            else:
                 if not data.get("å‚™è¨»"): data["å‚™è¨»"] = "ä¸€èˆ¬"
            
            final_results.append(data)
            
            if i % 3 == 0:
                df_show = pd.DataFrame(final_results)
                # é€™è£¡ä¹Ÿæ²’æœ‰çµ±ç·¨äº†
                cols = ["å…¬å¸åç¨±", "é›»è©±", "Email", "ç¶²å€"]
                for c in cols: 
                    if c not in df_show.columns: df_show[c] = ""
                table_preview.dataframe(df_show[cols].tail(5))
                
        except Exception as e:
            if debug_mode: st.warning(f"Error on {title}: {e}")
            
        process_bar.progress((i+1)/len(target_list))
        time.sleep(0.5)

    # 4. è¼¸å‡º
    status_box.update(label="ğŸ‰ å®Œæˆï¼", state="complete", expanded=False)
    
    if final_results:
        df_final = pd.DataFrame(final_results)
        
        # æœ€çµ‚æ¬„ä½æ•´ç† (ç„¡çµ±ç·¨)
        target_cols = ["å…¬å¸åç¨±", "é›»è©±", "Email", "å‚³çœŸ", "ç¶²å€", "å‚™è¨»", "è³‡æ–™ä¾†æº"]
        for c in target_cols:
            if c not in df_final.columns: df_final[c] = ""
        df_final = df_final[target_cols].astype(str)
        
        st.success(f"å…±ç”¢å‡º {len(df_final)} ç­†å°ç£å» å•†åå–®")
        st.dataframe(df_final)
        
        csv = df_final.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´åå–® (CSV)",
            data=csv,
            file_name="taiwan_leads_clean.csv",
            mime="text/csv"
        )