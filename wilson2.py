import streamlit as st
import pandas as pd
from tavily import TavilyClient
import google.generativeai as genai
import time
import json

# --- 1. åŸºç¤è¨­å®š ---
st.set_page_config(page_title="è¶…ç´šåå–®æœé›†å™¨ (AIè£‚è®Šç‰ˆ)", layout="wide")
st.title("ğŸ“Š ä¼æ¥­åå–®è‡ªå‹•æœé›† (AI é—œéµå­—è£‚è®Šç‰ˆ)")
st.markdown("å°ˆé–€è§£æ±ºã€Œç¯„åœå¤ªå»£ã€çš„å•é¡Œï¼šAI æœƒè‡ªå‹•å°‡å¤§é—œéµå­—æ‹†è§£æˆæ•¸åå€‹ç²¾æº–æœå°‹è©ï¼Œç¢ºä¿è³‡æ–™å¤šæ¨£æ€§ã€‚")

# --- 2. å´é‚Šæ¬„åƒæ•¸ ---
with st.sidebar:
    st.header("âš™ï¸ API è¨­å®š")
    tavily_api_key = st.text_input("Tavily API Key", type="password")
    gemini_api_key = st.text_input("Gemini API Key", type="password")
    
    st.divider()
    
    # ç¯„åœ 100 - 500
    target_limit = st.slider("ğŸ¯ ç›®æ¨™è³‡æ–™ç­†æ•¸", min_value=100, max_value=500, value=100, step=50)
    st.info("ğŸ’¡ æç¤ºï¼šè¨­å®šè¶Šé«˜ï¼ŒAI ç”Ÿæˆçš„æœå°‹ç­–ç•¥æœƒè¶Šè©³ç´°ã€‚")

# --- 3. ä¸»ç•«é¢ ---
col1, col2 = st.columns([3, 1])
with col1:
    search_query = st.text_input("æœå°‹é—œéµå­— (ä¾‹å¦‚ï¼šå»ºç¯‰æ¥­ã€é£Ÿå“æ¥­ã€å»¢æ°´è™•ç†)", value="å»¢æ°´å›æ”¶ç³»çµ±")
with col2:
    st.write(" ") 
    st.write(" ")
    start_btn = st.button("ğŸš€ AI è¦åŠƒä¸¦åŸ·è¡Œ", type="primary", use_container_width=True)

# --- 4. åŸ·è¡Œé‚è¼¯ ---
if start_btn:
    if not tavily_api_key or not gemini_api_key:
        st.error("âŒ è«‹è¼¸å…¥ API Key æ‰èƒ½åŸ·è¡Œï¼")
        st.stop()

    tavily = TavilyClient(api_key=tavily_api_key)
    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel('gemini-1.5-pro')

    # ==========================
    # éšæ®µé›¶ï¼šAI é—œéµå­—è£‚è®Š (æ–°å¢åŠŸèƒ½)
    # ==========================
    status_box = st.status("ğŸ§  AI æ­£åœ¨åˆ†æç”¢æ¥­çµæ§‹ä¸¦è¦åŠƒæœå°‹ç­–ç•¥...", expanded=True)
    
    # è¨ˆç®—éœ€è¦å¤šå°‘å€‹æœå°‹è© (Tavily ä¸€æ¬¡ç´„ 10-15 ç­†æœ‰æ•ˆï¼Œæ‰€ä»¥é™¤ä»¥ 10)
    needed_queries = int(target_limit / 10) + 5
    
    plan_prompt = f"""
    ä½¿ç”¨è€…æƒ³è¦æœå°‹é—œæ–¼ã€Œ{search_query}ã€çš„ä¼æ¥­åå–®ã€‚
    å› ç‚ºç¯„åœå¾ˆå»£ï¼Œè«‹ä½ å¹«æˆ‘æ‹†è§£å‡º {needed_queries} å€‹ã€Œå…·é«”ä¸”å¤šæ¨£åŒ–ã€çš„æœå°‹é—œéµå­—ï¼Œä»¥ä¾¿æ‰¾å‡ºè©²é ˜åŸŸä¸Šä¸­ä¸‹æ¸¸çš„ä¸åŒå…¬å¸ã€‚
    
    è«‹åŒ…å«ï¼š
    1. å…·é«”çš„è¨­å‚™åç¨± (ä¾‹å¦‚ï¼šROé€†æ»²é€ã€æ±™æ³¥å£“æ¿¾æ©Ÿ)
    2. å…·é«”çš„æœå‹™é¡å‹ (ä¾‹å¦‚ï¼šä»£æ“ã€ç’°ä¿å·¥ç¨‹ã€æª¢æ¸¬)
    3. ç›¸é—œçš„ä¾›æ‡‰éˆè§’è‰² (ä¾‹å¦‚ï¼šè£½é€ å•†ã€ä»£ç†å•†ã€ç¶“éŠ·å•†)
    4. çµåˆå°ç£ä¸»è¦å·¥æ¥­å€æˆ–åœ°å€ (ä¾‹å¦‚ï¼šç«¹ç§‘ å»¢æ°´è™•ç†ã€é«˜é›„ ç’°ä¿å…¬å¸)

    è«‹ç›´æ¥å›å‚³ä¸€å€‹ JSON String Arrayï¼Œä¾‹å¦‚ï¼š
    ["{search_query} è¨­å‚™å•†", "{search_query} å·¥ç¨‹å…¬å¸", "ç‰¹å®šæŠ€è¡“ å» å•†"...]
    
    æ³¨æ„ï¼šåªå›å‚³ JSON Arrayï¼Œä¸è¦æœ‰ Markdownã€‚
    """
    
    try:
        plan_res = model.generate_content(plan_prompt)
        plan_text = plan_res.text.replace("```json", "").replace("```", "").strip()
        search_keywords = json.loads(plan_text)
        
        status_box.write(f"âœ… ç­–ç•¥è¦åŠƒå®Œæˆï¼AI ç”Ÿæˆäº† {len(search_keywords)} çµ„ç²¾æº–æœå°‹è©ï¼š")
        status_box.json(search_keywords) # é¡¯ç¤ºå‡ºä¾†è®“ä½ çŸ¥é“ AI å¤šè°æ˜
        
    except Exception as e:
        status_box.warning(f"AI è¦åŠƒå¤±æ•—ï¼Œåˆ‡æ›å›é è¨­ç­–ç•¥: {e}")
        # å‚™ç”¨æ–¹æ¡ˆ
        search_keywords = [f"{search_query} {s}" for s in ["å» å•†", "å…¬å¸", "ä¾›æ‡‰å•†", "å·¥ç¨‹", "è¨­å‚™", "å°åŒ—", "å°ä¸­", "é«˜é›„"]]

    # ==========================
    # éšæ®µä¸€ï¼šä¾æ“š AI ç­–ç•¥é€²è¡Œæœå°‹
    # ==========================
    status_box.write("ğŸ“¡ é–‹å§‹åŸ·è¡Œå¤šåŸ·è¡Œç·’æœå°‹...")
    
    raw_results = []
    seen_urls = set()
    
    progress_bar = st.progress(0)
    
    # è¿´åœˆæŠ“å– (ä½¿ç”¨ AI ç”Ÿæˆçš„é—œéµå­—)
    for i, query in enumerate(search_keywords):
        # æª¢æŸ¥æ˜¯å¦é”æ¨™
        if len(raw_results) >= target_limit:
            break
            
        status_box.write(f"ğŸ” ({len(raw_results)}/{target_limit}) æ­£åœ¨æœå°‹ï¼š**{query}**")
        
        try:
            response = tavily.search(
                query=query,
                max_results=20, 
                search_depth="advanced"
            )
            
            items_found = 0
            for item in response.get('results', []):
                url = item.get('url')
                if url and url not in seen_urls:
                    raw_results.append(item) 
                    seen_urls.add(url)
                    items_found += 1
            
            time.sleep(0.5) 
            
        except Exception:
            continue
            
        # æ›´æ–°é€²åº¦ (å‰ 70% çµ¦æœå°‹)
        search_progress = min(len(raw_results) / target_limit, 1.0) * 0.7
        progress_bar.progress(search_progress)

    # æˆªæ–·å¤šé¤˜è³‡æ–™
    final_raw_data = raw_results[:target_limit]
    status_box.write(f"âœ… æœå°‹å®Œæˆï¼å…±å–å¾— {len(final_raw_data)} ç­†è³‡æ–™ã€‚é–‹å§‹ AI æ¬„ä½èƒå–...")

    # ==========================
    # éšæ®µäºŒï¼šGemini æ•´ç†æ¬„ä½
    # ==========================
    parsed_data = []
    batch_size = 15 
    
    if len(final_raw_data) > 0:
        total_batches = (len(final_raw_data) + batch_size - 1) // batch_size
        
        for i in range(0, len(final_raw_data), batch_size):
            batch = final_raw_data[i:i+batch_size]
            
            # è¨ˆç®—é€²åº¦ (å¾ 0.7 é–‹å§‹è·‘åˆ° 1.0)
            current_batch_idx = i // batch_size
            prog = 0.7 + 0.3 * (current_batch_idx / total_batches)
            progress_bar.progress(min(prog, 0.99))
            
            try:
                batch_json = json.dumps(batch, ensure_ascii=False)
                
                prompt = f"""
                è«‹å¾ JSON è³‡æ–™ä¸­æå–å…¬å¸è¯çµ¡è³‡è¨Šã€‚
                è¼¸å‡º JSON Arrayï¼ŒåŒ…å«ï¼š
                1. "å…¬å¸åç¨±"
                2. "Email" (ç„¡å‰‡ç©º)
                3. "å‚³çœŸ" (ç„¡å‰‡ç©º)
                4. "é›»è©±" (ç„¡å‰‡ç©º)
                5. "ç¶²å€" (ä½¿ç”¨ url)

                åŸå§‹è³‡æ–™:
                {batch_json}
                """
                
                res = model.generate_content(prompt)
                clean_json = res.text.replace("```json", "").replace("```", "").strip()
                
                try:
                    batch_result = json.loads(clean_json)
                    parsed_data.extend(batch_result)
                except:
                    for item in batch:
                        parsed_data.append({"å…¬å¸åç¨±": item.get('title'), "Email":"", "å‚³çœŸ":"", "é›»è©±":"", "ç¶²å€": item.get('url')})
                
            except:
                for item in batch:
                    parsed_data.append({"å…¬å¸åç¨±": item.get('title'), "Email":"", "å‚³çœŸ":"", "é›»è©±":"", "ç¶²å€": item.get('url')})
            
            time.sleep(1.0) 

    progress_bar.progress(1.0)
    status_box.update(label="ğŸ‰ è™•ç†å®Œæˆï¼", state="complete", expanded=False)

    # ==========================
    # éšæ®µä¸‰ï¼šç”¢å‡º Excel
    # ==========================
    df = pd.DataFrame(parsed_data)
    target_cols = ["å…¬å¸åç¨±", "Email", "å‚³çœŸ", "ç¶²å€", "é›»è©±"]
    for col in target_cols:
        if col not in df.columns: df[col] = ""
    df = df[target_cols]

    st.subheader(f"æª”æ¡ˆé è¦½ (å…± {len(df)} ç­†)")
    st.dataframe(df.head(), use_container_width=True)
    
    csv = df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è¼‰ Excel æª”æ¡ˆ ({len(df)}ç­†è³‡æ–™.csv)",
        data=csv,
        file_name=f'{search_query}_åå–®.csv',
        mime='text/csv',
        type="primary"
    )